
import numpy as np

import torch

# --------------------------------------------------------
# 2D sine-cosine position embedding
# References:
# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py
# MoCo v3: https://github.com/facebookresearch/moco-v3
# MAE: https://github.com/facebookresearch/mae
# --------------------------------------------------------
def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed

def get_2d_sincos_pos_embed_rectangle(embed_dim, grid_size,cls_token=False):
    """
    grid_size, a tuple of height and width
    """
    grid_size_h, grid_size_w = grid_size
    grid_h = np.arange(grid_size_h, dtype=np.float32)
    grid_w = np.arange(grid_size_w, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, grid_size_w, grid_size_h])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)

    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)
    return emb


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb

def convert_count_to_pos_embed(count, embed_dim):
    """
    #count should be log formated
    count: (N,1)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega = omega/ embed_dim / 2.
    omega = 1. / 10000**omega
    
    count = count.reshape(-1) # (N,)
    out = np.einsum('m,d->md', count, omega)  # (M, D/2), outer product
    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)+1 #enforce different compared to other embeddings
      # (M, D)
    return emb

def convert_count_to_pos_embed_cuda(count, embed_dim):
    """
    #count should be log formated
    count: (N,1)
    """
    assert embed_dim % 2 == 0
    omega = torch.arange(embed_dim // 2, dtype=count.dtype,device=count.device)
    omega = omega/ embed_dim / 2.
    omega = 1. / 10000**omega
    
    out = torch.einsum('m,d->md', count, omega)  # (M, D/2), outer product
    emb_sin = torch.sin(out) # (M, D/2)
    emb_cos = torch.cos(out) # (M, D/2)

    emb = torch.cat([emb_sin, emb_cos], dim=1)+1 #enforce different compared to other embeddings
      # (M, D)
    return emb


# --------------------------------------------------------
# Interpolate position embeddings for high-resolution
# References:
# DeiT: https://github.com/facebookresearch/deit
# --------------------------------------------------------
def interpolate_pos_embed(model, checkpoint_model,use_decoder=True):
    if 'pos_embed' in checkpoint_model and 'pos_embed' in model.state_dict():
        pos_embed_checkpoint = checkpoint_model['pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.pos_embed.shape[-2] - num_patches
        # height (== width) for the checkpoint position embedding
        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
        # height (== width) for the new position embedding
        new_size = int(num_patches ** 0.5)
        # class_token and dist_token are kept unchanged
        if orig_size != new_size:
            print("Position interpolate from %dx%d to %dx%d" % (orig_size, orig_size, new_size, new_size))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            # only the position tokens are interpolated
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['pos_embed'] = new_pos_embed
    if "decoder_pos_embed" in checkpoint_model and "decoder_pos_embed" in model.state_dict() and use_decoder ==True:
        pos_embed_checkpoint = checkpoint_model['decoder_pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.decoder_pos_embed.shape[-2] - num_patches
        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
        new_size = int(num_patches ** 0.5)
        if orig_size!=new_size:
            print("Position interpolate for decoder from %dx%d to %dx%d" % (orig_size, orig_size, new_size, new_size))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['decoder_pos_embed'] = new_pos_embed
    if "vit_backbone.pos_embed" in checkpoint_model and "vit_backbone.pos_embed" in model.state_dict():
        pos_embed_checkpoint = checkpoint_model['vit_backbone.pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        vit_backbone = model.vit_backbone
        num_patches = vit_backbone.patch_embed.num_patches
        num_extra_tokens = vit_backbone.pos_embed.shape[-2] - num_patches
        # height (== width) for the checkpoint position embedding
        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
        # height (== width) for the new position embedding
        new_size = int(num_patches ** 0.5)
        # class_token and dist_token are kept unchanged
        if orig_size != new_size:
            print("Position interpolate from %dx%d to %dx%d" % (orig_size, orig_size, new_size, new_size))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            # only the position tokens are interpolated
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['vit_backbone.pos_embed'] = new_pos_embed
    

def interpolate_pos_embed_inputsize(model, checkpoint_model,input_size=(16,4000),
                                    use_decoder=True):
    if 'pos_embed' in checkpoint_model and not use_decoder:
        pos_embed_checkpoint = checkpoint_model['pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.pos_embed.shape[-2] - num_patches  # e.g., 3 (CLS + count + seq)

        print(f"DEBUG → num_extra_tokens: {num_extra_tokens} (expected 3)")

        if model.use_sequence:
            # height (== width) for the checkpoint position embedding
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            orig_patch_count = pos_tokens.shape[1]
            orig_size = int(orig_patch_count ** 0.5)
            # height (== width) for the new position embedding
        else:
            orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)

        # class_token and dist_token are kept unchanged
        if orig_size != input_size[0] or orig_size!=input_size[1]:
            print("Position interpolate from %dx%d to %dx%d" % (orig_size, orig_size, input_size[0], input_size[1]))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            # only the position tokens are interpolated
            if not model.use_sequence: pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:] #----> moved above to use in orig_size calculation
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(input_size[0],input_size[1]), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['pos_embed'] = new_pos_embed
    if "decoder_pos_embed" in checkpoint_model and use_decoder:
        pos_embed_checkpoint = checkpoint_model['decoder_pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.decoder_pos_embed.shape[-2] - num_patches
        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
        if orig_size != input_size[0] or orig_size!=input_size[1]:
            print("Position interpolate for decoder from %dx%d to %dx%d" % (orig_size, orig_size, input_size[0], input_size[1]))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(input_size[0],input_size[1]), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['decoder_pos_embed'] = new_pos_embed

def expand_pos_embed_add_count_and_seq(pos_embed, embed_dim, device=None):
    """
    Expand pos_embed by adding count and sequence tokens after cls.

    Args:
        pos_embed (torch.Tensor): shape (1, old_len, embed_dim)
        embed_dim (int): embedding dimension
        device (torch.device, optional): target device

    Returns:
        torch.Tensor: expanded pos_embed with +2 tokens
    """
    B, old_len, D = pos_embed.shape

    cls = pos_embed[:, :1, :]         # (1, 1, D)
    patches = pos_embed[:, 1:, :]     # (1, N, D)

    # Create new tokens
    new_count_token = torch.randn(1, 1, D, device=device or pos_embed.device) * 0.02
    new_seq_token = torch.randn(1, 1, D, device=device or pos_embed.device) * 0.02

    # Combine: CLS + COUNT + SEQ + PATCHES
    expanded = torch.cat([cls, new_count_token, new_seq_token, patches], dim=1)

    print(f"Expanded pos_embed from {old_len} → {expanded.shape[1]} (CLS + COUNT + SEQ + patches)")

    return expanded
