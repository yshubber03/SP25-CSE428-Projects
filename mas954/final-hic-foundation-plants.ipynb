{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "19bf3e12-973e-4bae-9184-35920d20f753",
      "cell_type": "markdown",
      "source": "HiCFoundation Resolution Enhancement Pipeline for Google Colab\nThis notebook provides a complete pipeline for Hi-C resolution enhancement using HiCFoundation, optimized for Google Colab.\nPrerequisites\nBefore starting, make sure to:\n\nEnable GPU in Runtime → Change runtime type → Hardware accelerator → GPU (T4 or better)\nHave your .hic files ready to upload\nHave a Google Drive account with sufficient storage space",
      "metadata": {}
    },
    {
      "id": "ab738a2a-1052-4292-8a74-68686dbdba30",
      "cell_type": "markdown",
      "source": "1. Environment Setup\nCheck GPU and Mount Google Drive",
      "metadata": {}
    },
    {
      "id": "13085ff7-091a-4efb-b3c2-965d2a32d2c0",
      "cell_type": "code",
      "source": "# Check GPU availability\nimport torch\nimport os\n\nif torch.cuda.is_available():\n    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"WARNING: No GPU detected! Please enable GPU in Runtime settings.\")\n\n# Mount Google Drive for data storage\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create working directory in Google Drive\nDRIVE_PATH = '/content/drive/MyDrive/HiCFoundation'\nos.makedirs(DRIVE_PATH, exist_ok=True)\nos.chdir(DRIVE_PATH)\nprint(f\"Working directory: {os.getcwd()}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cd5b41f3-4fc1-4494-86c1-d4602ee87112",
      "cell_type": "markdown",
      "source": "Install Dependencies",
      "metadata": {}
    },
    {
      "id": "87f134d3-8ed5-4845-a766-2bd7cd949bab",
      "cell_type": "code",
      "source": "# Install PyTorch with CUDA support\n!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n\n# Install other required packages\n!pip install easydict opencv-python simplejson lvis Pillow==9.5.0 pytorch_msssim\n!pip install pandas hic-straw matplotlib scikit-image scipy einops tensorboard\n!pip install cooler numba pyBigWig timm==0.3.2 scikit-learn\n\n# Clone HiCFoundation repositories\n!git clone https://github.com/Noble-Lab/HiCFoundation.git\n!git clone https://github.com/Noble-Lab/HiCFoundation_paper.git\n\n# Copy necessary files from HiCFoundation repo\n!cp -r HiCFoundation/* .\n!cp -r HiCFoundation_paper/utils/* utils/ 2>/dev/null || true",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e757b77a-2671-4311-9252-096658d5c7f9",
      "cell_type": "markdown",
      "source": "Create Directory Structure",
      "metadata": {}
    },
    {
      "id": "f1446044-68bb-4a03-9040-d3c589d2410a",
      "cell_type": "code",
      "source": "import os\n\n# Create necessary directories\ndirs_to_create = [\n    'utils',\n    'hic-raw',\n    'input-dirs',\n    'input-dirs/pre-train-dirs',\n    'ft-inputs',\n    'ft-inputs/train',\n    'ft-inputs/val',\n    'outputs',\n    'models',\n    'logs'\n]\n\nfor dir_name in dirs_to_create:\n    os.makedirs(dir_name, exist_ok=True)\n    print(f\"Created directory: {dir_name}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff934b05-97ff-4b77-8768-4043c3a7139b",
      "cell_type": "markdown",
      "source": "2. Data Upload\nGo to this link: https://drive.google.com/drive/folders/1D5MqwauHKRFixhRbGljSnouxWFNVfL1l?usp=sharing\nDownload and Upload the .hic Files",
      "metadata": {}
    },
    {
      "id": "96825184-eaa8-4bb7-b016-53d9bec4ca87",
      "cell_type": "code",
      "source": "from google.colab import files\nimport shutil\n\nprint(\"Upload your .hic files:\")\nuploaded = files.upload()\n\n# Move uploaded files to hic-raw directory\nfor filename in uploaded.keys():\n    shutil.move(filename, f'hic-raw/{filename}')\n    print(f\"Moved {filename} to hic-raw/\")\n\n# List files in hic-raw directory\n!ls -la hic-raw/",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b0b7d7d5-5442-4036-a6d8-176d11f70d96",
      "cell_type": "markdown",
      "source": "3. Data Preprocessing\nCreate hic2array.py",
      "metadata": {}
    },
    {
      "id": "4291d455-f54e-447d-b4af-16d5068e897a",
      "cell_type": "code",
      "source": "%%writefile utils/hic2array.py\nimport numpy as np\nfrom scipy.sparse import coo_matrix\nimport hicstraw\nimport os\nimport pickle\n\ndef write_pkl(data, path):\n    with open(path, 'wb') as f:\n        pickle.dump(data, f)\n\ndef read_chrom_array(chr1, chr2, normalization, hic_file, resolution):\n    chr1_name = chr1.name\n    chr2_name = chr2.name\n    infos = []\n    infos.append('observed')\n    infos.append(normalization)\n    infos.append(hic_file)\n    infos.append(chr1_name)\n    infos.append(chr2_name)\n    infos.append('BP')\n    infos.append(resolution)\n    print(infos)\n    row, col, val = [], [], []\n    rets = hicstraw.straw(*infos)\n    print('\\tlen(rets): {:3e}'.format(len(rets)))\n    for ret in rets:\n        row.append((int)(ret.binX // resolution))\n        col.append((int)(ret.binY // resolution))\n        val.append(ret.counts)\n    print('\\tsum(val): {:3e}'.format(sum(val)))\n    if sum(val) == 0:\n        return None\n    if chr1_name==chr2_name:\n        max_shape =max(max(row),max(col))+1\n        mat_coo = coo_matrix((val, (row, col)), shape = (max_shape,max_shape),dtype=np.float32)\n    else:\n        max_row = max(row)+1\n        max_column = max(col)+1\n        mat_coo = coo_matrix((val, (row, col)), shape = (max_row,max_column),dtype=np.float32)\n\n    mat_coo = mat_coo #+ triu(mat_coo, 1).T #no below diagonaline records\n\n    return mat_coo\n\n\ndef hic2array(input_hic,output_pkl=None,\n              resolution=25000,normalization=\"NONE\",\n              tondarray=0):\n    \"\"\"\n    input_hic: str, input hic file path\n    output_pkl: str, output pickle file path\n    resolution: int, resolution of the hic file\n    \"\"\"\n\n    hic = hicstraw.HiCFile(input_hic)\n    chrom_list=[]\n    chrom_dict={}\n    for chrom in hic.getChromosomes():\n        print(chrom.name, chrom.length)\n        if \"all\" in chrom.name.lower():\n            continue\n        chrom_list.append(chrom)\n        chrom_dict[chrom.name]=chrom.length\n    resolution_list = hic.getResolutions()\n    if resolution not in resolution_list:\n        print(\"Resolution not found in the hic file, please choose from the following list:\")\n        print(resolution_list)\n        exit()\n    output_dict={}\n    for i in range(len(chrom_list)):\n        for j in range(i,len(chrom_list)):\n            if i!=j and tondarray in [2,3]:\n                #skip inter-chromosome region\n                continue\n            \n            chrom1 = chrom_list[i]\n            chrom1_name = chrom_list[i].name\n            chrom2 = chrom_list[j]\n            chrom2_name = chrom_list[j].name\n            if 'Un' in chrom1_name or 'Un' in chrom2_name:\n                continue\n            if \"random\" in chrom1_name.lower() or \"random\" in chrom2_name.lower():\n                continue\n            if \"alt\" in chrom1_name.lower() or \"alt\" in chrom2_name.lower():\n                continue\n            read_array=read_chrom_array(chrom1,chrom2, normalization, input_hic, resolution)\n            if read_array is None:\n                print(\"No data found for\",chrom1_name,chrom2_name)\n                continue\n            if tondarray in [1,3]:\n                read_array = read_array.toarray()\n            if tondarray in [2,3]:\n                output_dict[chrom1_name]=read_array\n            else:\n                output_dict[chrom1_name+\"_\"+chrom2_name]=read_array\n    if output_pkl is not None:\n        output_dir = os.path.dirname(os.path.realpath(output_pkl))\n        os.makedirs(output_dir, exist_ok=True)\n        write_pkl(output_dict,output_pkl)\n\n    return output_dict\n\nif __name__ == '__main__':\n    import os \n    import sys\n    if len(sys.argv) != 6:\n        print('Usage: python3 hic2array.py [input.hic] [output.pkl] [resolution] [normalization_type] [mode]')\n        print(\"This is the full hic2array script. \")\n        print(\"normalization type: 0: None normalization; 1: VC normalization; 2: VC_SQRT normalization; 3: KR normalization; 4: SCALE normalization\")\n        print(\"mode: 0 for sparse matrix, 1 for dense matrix, 2 for sparce matrix (only cis-contact); 3 for dense matrix (only cis-contact).\")\n        sys.exit(1)\n    resolution = int(sys.argv[3])\n    normalization_type = int(sys.argv[4])\n    mode = int(sys.argv[5])\n    normalization_dict={0:\"NONE\",1:\"VC\",2:\"VC_SQRT\",3:\"KR\",4:\"SCALE\"}\n    if normalization_type not in normalization_dict:\n        print('normalization type should be 0,1,2,3,4')\n        print(\"normalization type: 0: None normalization; 1: VC normalization; 2: VC_SQRT normalization; 3: KR normalization; 4: SCALE normalization\")\n        sys.exit(1)\n    normalization_type = normalization_dict[normalization_type]\n    if mode not in [0,1,2,3]:\n        print('mode should be in choice of 0/1/2/3')\n        print(\"mode: 0 for sparse matrix, 1 for dense matrix, 2 for sparce matrix (only cis-contact); 3 for dense matrix (only cis-contact).\")\n        sys.exit(1)\n    input_hic_path = os.path.abspath(sys.argv[1])\n    output_pkl_path = os.path.abspath(sys.argv[2])\n    output_dir = os.path.dirname(output_pkl_path)\n    os.makedirs(output_dir,exist_ok=True)\n    hic2array(input_hic_path,output_pkl_path,resolution,normalization_type,mode)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "18eb0e55-f3fc-453f-b8ad-e2dfb02d65f0",
      "cell_type": "markdown",
      "source": "Convert .hic Files to .pkl Format",
      "metadata": {}
    },
    {
      "id": "9e9ac349-4d27-4f34-a449-d52efcbd6deb",
      "cell_type": "code",
      "source": "# List available .hic files\nimport glob\nhic_files = glob.glob('hic-raw/*.hic')\nprint(\"Available .hic files:\")\nfor f in hic_files:\n    print(f\"  - {f}\")\n\n# Convert each file (update filenames as needed)\n# Example conversions:\n!python3 utils/hic2array.py hic-raw/Ft1-GSM6077013_at_hic_ndx1-4_r2.hic Ftr1.pkl 25000 0 0\n!python3 utils/hic2array.py hic-raw/Pt1-GSM4705443_ddcc.hic Ptr1.pkl 25000 0 0\n!python3 utils/hic2array.py hic-raw/Pt2-GSM6077012_at_hic_ndx1-4_r1.hic Ptr2.pkl 25000 0 0\n!python3 utils/hic2array.py hic-raw/Pv1-GSM5091844_S_WT_2h1_DNB-15.allValidPairs.hic Pv1.pkl 25000 0 0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "076f9fda-fe2c-4163-a7c3-cbfd3238ea2c",
      "cell_type": "markdown",
      "source": "4. Submatrix Generation\nCreate scan_array.py",
      "metadata": {}
    },
    {
      "id": "1c577dfa-6646-4919-86ad-d9604e94a538",
      "cell_type": "code",
      "source": "%%writefile utils/scan_array.py\nimport numpy as np\nimport pickle\nfrom scipy.sparse import coo_matrix\nimport os\n\ndef write_pickle(output_dict,output_path):\n    \"\"\"\n    output_dict: dict, output dictionary\n    output_path: str, output path\n    \"\"\"\n    with open(output_path, 'wb') as f:\n        pickle.dump(output_dict, f)\n\ndef scan_matrix(matrix, input_row_size,input_col_size, stride_row,\n                stride_col,hic_count,output_dir,current_chrom,\n                filter_threshold=0.05):\n    \"\"\"\n    matrix: 2D array\n    input_row_size: int, row size of scanned output submatrix\n    input_col_size: int, column size of scanned output submatrix\n    stride_row: int, row stride\n    stride_col: int, column stride\n    hic_count: int, total read count of the Hi-C experiments\n    output_dir: str, output directory\n    current_chrom: str, current chromosome\n    \"\"\"\n    row_size = matrix.shape[0]\n    col_size = matrix.shape[1]\n    count_save=0\n    region_size = input_row_size * input_col_size\n    for i in range(0, row_size - input_row_size//2, stride_row):\n        for j in range(0, col_size - input_col_size//2, stride_col):\n            submatrix = np.zeros((input_row_size, input_col_size))\n            row_start = max(0,i)\n            row_end = min(row_size, i + input_row_size)\n            col_start = max(0,j)\n            col_end = min(col_size, j + input_col_size)\n            submatrix[:row_end-row_start,:col_end-col_start] = matrix[row_start: row_end, col_start: col_end]\n            #filter out the submatrices with too many zeros\n            count_useful = np.count_nonzero(submatrix)\n            if count_useful < region_size * filter_threshold:\n                continue\n            \n            output_dict={}\n            output_dict['input']=submatrix\n            output_dict['input_count']=hic_count\n            #judge if the diag is possibly included\n            if col_start < row_start and col_end >row_start:\n                output_dict['diag']=abs (col_start-row_start)\n            elif col_start == row_start:\n                output_dict['diag']=0\n            elif col_start> row_start and col_start < row_end:\n                output_dict['diag']= -abs (col_start-row_start)\n            else:\n                output_dict['diag']=None\n            output_path = os.path.join(output_dir, str(current_chrom) + '_' + str(i) + '_' + str(j) + '.pkl')\n            write_pickle(output_dict,output_path)\n            count_save+=1\n            if count_save%100==0:\n                print('Processed %d submatrices' % count_save, \" for chromosome \", current_chrom)\n        \n    return \n\ndef scan_pickle(input_pkl_path, input_row_size,input_col_size, stride_row,\n                stride_col,output_dir,filter_threshold):\n    \"\"\"\n    input_pkl_path: str, input pickle path  \n    input_row_size: int, row size of scanned output submatrix\n    input_col_size: int, column size of scanned output submatrix\n    stride_row: int, row stride\n    stride_col: int, column stride\n    output_dir: str, output directory\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    with open(input_pkl_path, 'rb') as f:\n        data = pickle.load(f)\n    total_count = 0\n    for key in data:\n        matrix = data[key]\n        if isinstance(matrix, np.ndarray):\n            cur_count = np.sum(matrix)\n        elif isinstance(matrix, coo_matrix):\n            cur_count = matrix.sum()\n        else:\n            print(\"Type not supported\", type(matrix))\n            exit()\n        total_count += cur_count\n    print(\"Total read count of Hi-C: \", total_count)        \n\n    for key in data:\n        matrix = data[key]\n        if isinstance(matrix, coo_matrix):\n            matrix = matrix.toarray()\n            \n            if matrix.shape[0]==matrix.shape[1]:\n                #intra chromosmoe\n                #get the symmetrical one \n                upper_tri = np.triu(matrix,1)\n                all_triu = np.triu(matrix)\n                matrix = all_triu + upper_tri.T\n            else:\n                matrix = matrix\n        current_chrom = str(key)\n        if \"chr\" not in current_chrom:\n            current_chrom = \"chr\" + current_chrom\n\n        scan_matrix(matrix, input_row_size,input_col_size, stride_row,\n                stride_col,total_count,output_dir,current_chrom,filter_threshold)\n\n#run with the simple command line\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_pkl_path', type=str, required=True)\n    parser.add_argument('--input_row_size', type=int, required=True)\n    parser.add_argument('--input_col_size', type=int, required=True)\n    parser.add_argument('--stride_row', type=int, required=True)\n    parser.add_argument('--stride_col', type=int, required=True)\n    parser.add_argument('--output_dir', type=str, required=True)\n    parser.add_argument('--filter_threshold', type=float, default=0.05)\n    args = parser.parse_args()\n    input_pkl_path = os.path.abspath(args.input_pkl_path)\n    output_dir = os.path.abspath(args.output_dir)\n    scan_pickle(input_pkl_path, args.input_row_size, args.input_col_size, \n                args.stride_row, args.stride_col, output_dir, args.filter_threshold)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f50a9fb-eec5-4e2c-85f0-8b0eeba77eeb",
      "cell_type": "markdown",
      "source": "Generate Submatrices for Pre-training",
      "metadata": {}
    },
    {
      "id": "2c225422-d2aa-4f90-b6cc-3ad75a3e72f7",
      "cell_type": "code",
      "source": "# Generate submatrices for pre-training\n!python3 utils/scan_array.py --input_pkl_path Ptr1.pkl  --input_row_size 448 \\\n    --input_col_size 448 --stride_row 224 --stride_col 224 \\\n    --output_dir HiC-PTR1 --filter_threshold 0.01\n\n!python3 utils/scan_array.py --input_pkl_path Ptr2.pkl  --input_row_size 448 \\\n    --input_col_size 448 --stride_row 224 --stride_col 224 \\\n    --output_dir HiC-PTR2 --filter_threshold 0.01\n\n!python3 utils/scan_array.py --input_pkl_path Pv1.pkl  --input_row_size 448 \\\n    --input_col_size 448 --stride_row 224 --stride_col 224 \\\n    --output_dir HiC-PV1 --filter_threshold 0.01",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a7c792c-a528-43c4-ac80-92f7dfc15e06",
      "cell_type": "markdown",
      "source": "Create Configuration Files",
      "metadata": {}
    },
    {
      "id": "a358175f-231e-4d5e-a88f-c31545bfbfc9",
      "cell_type": "code",
      "source": "# Create train.txt\nwith open('input-dirs/pre-train-dirs/train.txt', 'w') as f:\n    f.write('HiC-PTR1\\n')\n    f.write('HiC-PTR2\\n')\n\n# Create val.txt\nwith open('input-dirs/pre-train-dirs/val.txt', 'w') as f:\n    f.write('HiC-PV1\\n')\n\nprint(\"Configuration files created successfully!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "49deccaa-aea9-43dc-b3de-f11a0971808a",
      "cell_type": "markdown",
      "source": "5. Pre-training\nRun Pre-training",
      "metadata": {}
    },
    {
      "id": "8d826ea9-5ee3-4403-a208-64ef694e46d0",
      "cell_type": "code",
      "source": "# Note: This will take considerable time\n!python3 pretrain.py --batch_size 1 --accum_iter 4 \\\n    --epochs 1 --warmup_epochs 1 --pin_mem \\\n    --mask_ratio 0.75 --sparsity_ratio 0.05 \\\n    --blr 1.5e-4 --min_lr 1e-7 --weight_decay 0.05 \\\n    --model \"vit_large_patch16\" --loss_alpha 1 --seed 888 \\\n    --data_path \"input-dirs/pre-train-dirs/\" --train_config \"train.txt\" \\\n    --valid_config \"val.txt\" --output \"hicfoundation_finetune\" \\\n    --tensorboard 1 --world_size 1 --dist_url \"tcp://localhost:10001\" --rank 0 \\\n    --input_row_size 448 --input_col_size 448 --patch_size 16 \\\n    --print_freq 1 --save_freq 1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3687e5c4-fd39-4a86-baad-76494e6644c4",
      "cell_type": "markdown",
      "source": "Rename Output Directory",
      "metadata": {}
    },
    {
      "id": "dfd43513-7c6b-4f14-8cf3-737c081d2944",
      "cell_type": "code",
      "source": "!mv hicfoundation_finetune hicfoundation_pretrain",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5498c1cb-42eb-4d71-a51f-4c892a44e6eb",
      "cell_type": "markdown",
      "source": "6. Fine-tuning Preparation\nCreate downsample_pkl.py",
      "metadata": {}
    },
    {
      "id": "c83c6332-f209-4e22-81ed-49b338048ef8",
      "cell_type": "code",
      "source": "%%writefile utils/downsample_pkl.py\nimport sys\nimport os\nfrom collections import defaultdict\nimport pickle\nimport numpy as np\nfrom scipy.sparse import coo_matrix\n\ndef array_to_coo(array):\n    \"\"\"\n    Convert a regular 2D NumPy array to a scipy.sparse.coo_matrix.\n\n    Parameters:\n    - array (numpy.ndarray): The input 2D array.\n\n    Returns:\n    - scipy.sparse.coo_matrix: The converted COO matrix.\n    \"\"\"\n    # Find the non-zero elements in the array\n    row, col = np.nonzero(array)\n\n    # Get the values of the non-zero elements\n    data = array[row, col]\n\n    # Create the COO matrix\n    coo_mat = coo_matrix((data, (row, col)), shape=array.shape)\n\n    return coo_mat\n\ndef sparse2tag(coo_mat):\n    tag_len = coo_mat.sum()\n    tag_len = int(tag_len)\n    tag_mat = np.zeros((tag_len, 2))\n    tag_mat = tag_mat.astype(int)\n    row, col, data = coo_mat.row, coo_mat.col, coo_mat.data\n    start_idx = 0\n    for i in range(len(row)):\n        end_idx = start_idx + int(data[i])\n        tag_mat[start_idx:end_idx, :] = (row[i], col[i])\n        start_idx = end_idx\n    return tag_mat, tag_len\n\ndef tag2sparse(tag, nsize):\n    \"\"\"\n    Coverts a coo-based tag matrix to sparse matrix.\n    \"\"\"\n    coo_data, data = np.unique(tag, axis=0, return_counts=True)\n    row, col = coo_data[:, 0], coo_data[:, 1]\n    sparse_mat = coo_matrix((data, (row, col)), shape=(nsize, nsize))\n    return sparse_mat\n\ndef downsampling_sparce(matrix, down_ratio, verbose=False):\n    \"\"\"\n    Downsampling method for sparse matrix.\n    \"\"\"\n    if verbose: print(f\"[Downsampling] Matrix shape is {matrix.shape}\")\n    tag_mat, tag_len = sparse2tag(matrix)\n    sample_idx = np.random.choice(tag_len, int(tag_len *down_ratio))\n    sample_tag = tag_mat[sample_idx]\n    if verbose: print(f'[Downsampling] Sampling {down_ratio} of {tag_len} reads')\n    down_mat = tag2sparse(sample_tag, matrix.shape[0])\n    return down_mat\n\n\ndef downsample_pkl(input_pkl, output_pkl, downsample_rate):\n    data = pickle.load(open(input_pkl, 'rb'))\n    return_dict={}\n    for chrom in data:\n        current_data = data[chrom]\n        if current_data.shape[0] <=100:\n            continue\n        #if it is numpy array convert to sparse matrix\n        if isinstance(current_data, np.ndarray):\n            current_data = array_to_coo(current_data)\n            \n        downsampled_data = downsampling_sparce(current_data, downsample_rate,verbose=1)\n        return_dict[chrom] = downsampled_data\n    pickle.dump(return_dict, open(output_pkl, \"wb\"))\n    print(\"finish downsampling %s\"%output_pkl)\n\nif __name__ == '__main__':\n    if len(sys.argv)!=4:\n        print(\"Usage: python3 downsample_pkl.py [input.pkl] [output.pkl] [downsample_rate]\")\n        print(\"This script is used to downsample the input pickle file.\")\n        print(\"[input.pkl]: the input pickle file\")\n        print(\"[output.pkl]: the output pickle file\")\n        print(\"[downsample_rate]: the downsample rate [float].\")\n        sys.exit(1)\n    input_pkl = os.path.abspath(sys.argv[1])\n    output_pkl = os.path.abspath(sys.argv[2])\n    output_dir = os.path.dirname(output_pkl)\n    os.makedirs(output_dir, exist_ok=True)    \n    downsample_rate = float(sys.argv[3])\n    downsample_pkl(input_pkl, output_pkl, downsample_rate)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de963b66-4890-42f9-a1a7-94c7b41a1a86",
      "cell_type": "markdown",
      "source": "Downsample Data",
      "metadata": {}
    },
    {
      "id": "695939e1-5570-42c7-a2d1-0eed2c8e5fdc",
      "cell_type": "code",
      "source": "!python3 utils/downsample_pkl.py Ftr1.pkl Ftr1_downsampled.pkl 0.1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "07d001d4-b026-408b-bbef-b73997165117",
      "cell_type": "markdown",
      "source": "Create scan_array_diag.py",
      "metadata": {}
    },
    {
      "id": "1b23454e-a318-4fd6-99a8-b38a2bd60bed",
      "cell_type": "code",
      "source": "%%writefile utils/scan_array_diag.py\nimport numpy as np\nimport pickle\nfrom scipy.sparse import coo_matrix\nimport os\n\ndef write_pickle(output_dict,output_path):\n    \"\"\"\n    output_dict: dict, output dictionary\n    output_path: str, output path\n    \"\"\"\n    with open(output_path, 'wb') as f:\n        pickle.dump(output_dict, f)\n\ndef scan_matrix_paired(original_matrix, downsampled_matrix, input_row_size, input_col_size, stride,\n                      hic_count, output_dir, current_chrom):\n    \"\"\"\n    original_matrix: 2D array, original high-quality Hi-C matrix\n    downsampled_matrix: 2D array, downsampled low-quality Hi-C matrix\n    input_row_size: int, row size of scanned output submatrix\n    input_col_size: int, column size of scanned output submatrix\n    stride: int, row stride\n    hic_count: int, total read count of the Hi-C experiments\n    output_dir: str, output directory\n    current_chrom: str, current chromosome\n    \"\"\"\n    row_size = original_matrix.shape[0]\n    col_size = original_matrix.shape[1]\n    count_save = 0\n    \n    # Ensure both matrices have the same dimensions\n    assert original_matrix.shape == downsampled_matrix.shape, \\\n        f\"Matrix shapes don't match: {original_matrix.shape} vs {downsampled_matrix.shape}\"\n    \n    print(f\"Scanning matrix {current_chrom} with shape {original_matrix.shape}\")\n    print(f\"Submatrix size: {input_row_size}x{input_col_size}, stride: {stride}\")\n    \n    # For rectangular matrices, scan with different patterns\n    if row_size == col_size:\n        # Square matrix: use diagonal scanning\n        for i in range(0, row_size - input_row_size + 1, stride):\n            j = i  # Diagonal scanning\n            if j + input_col_size > col_size:\n                continue\n                \n            original_submatrix = original_matrix[i:i+input_row_size, j:j+input_col_size]\n            downsampled_submatrix = downsampled_matrix[i:i+input_row_size, j:j+input_col_size]\n            \n            # Filter out submatrices with too many zeros\n            count_useful = np.count_nonzero(original_submatrix)\n            if count_useful < 1:\n                continue\n            \n            # Create paired output dictionary\n            output_dict = {}\n            output_dict['input'] = downsampled_submatrix.copy()\n            output_dict['2d_target'] = original_submatrix.copy()\n            output_dict['input_count'] = hic_count\n            \n            output_path = os.path.join(output_dir, str(current_chrom) + '_' + str(i) + '_' + str(j) + '.pkl')\n            write_pickle(output_dict, output_path)\n            count_save += 1\n            \n            if count_save % 100 == 0:\n                print('Processed %d paired submatrices' % count_save, \" for chromosome \", current_chrom)\n    else:\n        # Rectangular matrix: scan all possible positions\n        for i in range(0, row_size - input_row_size + 1, stride):\n            for j in range(0, col_size - input_col_size + 1, stride):\n                original_submatrix = original_matrix[i:i+input_row_size, j:j+input_col_size]\n                downsampled_submatrix = downsampled_matrix[i:i+input_row_size, j:j+input_col_size]\n                \n                # Filter out submatrices with too many zeros\n                count_useful = np.count_nonzero(original_submatrix)\n                if count_useful < 1:\n                    continue\n                \n                # Create paired output dictionary\n                output_dict = {}\n                output_dict['input'] = downsampled_submatrix.copy()\n                output_dict['2d_target'] = original_submatrix.copy()\n                output_dict['input_count'] = hic_count\n                \n                output_path = os.path.join(output_dir, str(current_chrom) + '_' + str(i) + '_' + str(j) + '.pkl')\n                write_pickle(output_dict, output_path)\n                count_save += 1\n                \n                if count_save % 100 == 0:\n                    print('Processed %d paired submatrices' % count_save, \" for chromosome \", current_chrom)\n    \n    print(f\"Total submatrices saved for {current_chrom}: {count_save}\")\n    return \n\ndef scan_pickle_paired(original_pkl_path, downsampled_pkl_path, input_row_size, input_col_size, \n                      stride, output_dir):\n    \"\"\"\n    original_pkl_path: str, path to original (high-quality) pickle file\n    downsampled_pkl_path: str, path to downsampled (low-quality) pickle file  \n    input_row_size: int, row size of scanned output submatrix\n    input_col_size: int, column size of scanned output submatrix\n    stride: int, row stride\n    output_dir: str, output directory\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load both pickle files\n    with open(original_pkl_path, 'rb') as f:\n        original_data = pickle.load(f)\n    \n    with open(downsampled_pkl_path, 'rb') as f:\n        downsampled_data = pickle.load(f)\n    \n    # Ensure both datasets have the same chromosomes\n    assert set(original_data.keys()) == set(downsampled_data.keys()), \\\n        \"Original and downsampled data must have the same chromosomes\"\n    \n    # Calculate total count from original data\n    total_count = 0\n    for key in original_data:\n        matrix = original_data[key]\n        if isinstance(matrix, np.ndarray):\n            cur_count = np.sum(matrix)\n        elif isinstance(matrix, coo_matrix):\n            cur_count = matrix.sum()\n        else:\n            print(\"Type not supported\", type(matrix))\n            exit()\n       total_count += cur_count\n   print(\"Total read count of original Hi-C: \", total_count)        \n\n   # Process each chromosome\n   for key in original_data:\n       original_matrix = original_data[key]\n       downsampled_matrix = downsampled_data[key]\n       \n       # Convert sparse matrices to dense arrays\n       if isinstance(original_matrix, coo_matrix):\n           original_matrix = original_matrix.toarray()\n       \n       if isinstance(downsampled_matrix, coo_matrix):\n           downsampled_matrix = downsampled_matrix.toarray()\n       \n       current_chrom = str(key)\n       if \"chr\" not in current_chrom:\n           current_chrom = \"chr\" + current_chrom\n       \n       # Only apply symmetry operation if matrix is square\n       if original_matrix.shape[0] == original_matrix.shape[1]:\n           # Get the symmetrical matrix for square matrices\n           upper_tri = np.triu(original_matrix, 1)\n           all_triu = np.triu(original_matrix)\n           original_matrix = all_triu + upper_tri.T\n           \n           upper_tri = np.triu(downsampled_matrix, 1)\n           all_triu = np.triu(downsampled_matrix)\n           downsampled_matrix = all_triu + upper_tri.T\n       else:\n           print(f\"Warning: Matrix for {current_chrom} is not square ({original_matrix.shape}). Skipping symmetry operation.\")\n\n       print(f\"Processing chromosome {current_chrom}\")\n       print(f\"Original matrix shape: {original_matrix.shape}\")\n       print(f\"Downsampled matrix shape: {downsampled_matrix.shape}\")\n\n       scan_matrix_paired(original_matrix, downsampled_matrix, input_row_size, input_col_size, \n                         stride, total_count, output_dir, current_chrom)\n\n# Run with the simple command line\nif __name__ == '__main__':\n   import argparse\n   parser = argparse.ArgumentParser()\n   parser.add_argument('--original_pkl_path', type=str, required=True, \n                      help='Path to original (high-quality) pickle file')\n   parser.add_argument('--downsampled_pkl_path', type=str, required=True,\n                      help='Path to downsampled (low-quality) pickle file')\n   parser.add_argument('--input_row_size', type=int, required=True)\n   parser.add_argument('--input_col_size', type=int, required=True)\n   parser.add_argument('--stride', type=int, required=True)\n   parser.add_argument('--output_dir', type=str, required=True)\n   args = parser.parse_args()\n   \n   original_pkl_path = os.path.abspath(args.original_pkl_path)\n   downsampled_pkl_path = os.path.abspath(args.downsampled_pkl_path)\n   output_dir = os.path.abspath(args.output_dir)\n   \n   scan_pickle_paired(original_pkl_path, downsampled_pkl_path, args.input_row_size, \n                     args.input_col_size, args.stride, output_dir)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cc3abcfd-088b-4640-a703-c219d7340c44",
      "cell_type": "markdown",
      "source": "Generate Paired Submatrices",
      "metadata": {}
    },
    {
      "id": "3a9d952b-fc9b-4cc0-8adc-3e9894ecd941",
      "cell_type": "code",
      "source": "!python3 utils/scan_array_diag.py \\\n    --original_pkl_path Ftr1.pkl \\\n    --downsampled_pkl_path Ftr1_downsampled.pkl \\\n    --input_row_size 224 --input_col_size 224 --stride 20 \\\n    --output_dir Ftr1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c1a935ec-e336-4364-9879-8d501eea26f2",
      "cell_type": "markdown",
      "source": "Prepare Fine-tuning Data",
      "metadata": {}
    },
    {
      "id": "e47e8f82-e03c-4fd0-94ed-1f1765d28be2",
      "cell_type": "code",
      "source": "import glob\nimport random\nimport shutil\n\n# Get all pkl files from Ftr1 directory\nftr1_files = glob.glob('Ftr1/*.pkl')\n\n# Shuffle and split (80-20 split)\nrandom.shuffle(ftr1_files)\nsplit_idx = int(0.8 * len(ftr1_files))\n\ntrain_files = ftr1_files[:split_idx]\nval_files = ftr1_files[split_idx:]\n\n# Copy files to respective directories\nfor f in train_files:\n    shutil.copy(f, 'ft-inputs/train/')\nfor f in val_files:\n    shutil.copy(f, 'ft-inputs/val/')\n\n# Create configuration files\nwith open('ft-inputs/train_config.txt', 'w') as f:\n    f.write('train\\n')\n\nwith open('ft-inputs/val_config.txt', 'w') as f:\n    f.write('val\\n')\n\nprint(f\"Created fine-tuning dataset: {len(train_files)} train, {len(val_files)} validation samples\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "663324c2-6209-44f2-94fb-e3ef06dbe738",
      "cell_type": "markdown",
      "source": "7. Fine-tuning\nCreate Modified train_epoch.py",
      "metadata": {}
    },
    {
      "id": "71659f16-be8e-4aab-8e33-e0f2b0043e9e",
      "cell_type": "code",
      "source": "%%writefile finetune/train_epoch.py\nimport math\nimport sys\nimport numpy as np\nfrom typing import Iterable\nimport torch\nimport torch.nn.functional as F\nimport time\n\nfrom ops.Logger import MetricLogger,SmoothedValue\nimport model.lr_sched as lr_sched\nfrom finetune.loss import configure_loss\nfrom ops.train_utils import list_to_device, to_value, create_image, torch_to_nparray, convert_gray_rgbimage\n\n\ndef train_epoch(model, data_loader_train, optimizer, \n                loss_scaler, epoch, device,\n                log_writer=None, args=None):\n    model.train()\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = args.print_freq\n\n    accum_iter = args.accum_iter\n\n    optimizer.zero_grad()\n    if log_writer is not None:\n        print('Tensorboard log dir: {}'.format(log_writer.log_dir))\n    print(\"number of iterations: \",len(data_loader_train))\n    criterion = configure_loss(args)\n\n    num_iter = len(data_loader_train)\n    for data_iter_step, train_data in enumerate(metric_logger.log_every(data_loader_train, print_freq, header)):\n        if data_iter_step % accum_iter == 0:\n            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader_train) + epoch, args)\n        input_matrix, total_count, target_matrix, embed_target, target_vector = list_to_device(train_data,device=device)\n        \n        # Forward pass\n        output_embedding, output_2d, output_1d = model(input_matrix, total_count)\n        \n        # Calculate losses - ensure all outputs participate in loss calculation\n        loss_components = []\n        \n        if embed_target is not None:\n            embedding_loss = criterion(output_embedding, embed_target)\n            loss_components.append(embedding_loss)\n        else:\n            # Use a small multiplier on the output to ensure gradients flow\n            # but don't affect the actual loss value\n            embedding_loss = 0.0 * output_embedding.mean()\n            loss_components.append(embedding_loss)\n            \n        if target_matrix is not None:\n            #flatten 2d matrix\n            output_2d_flatten = torch.flatten(output_2d, start_dim=1,end_dim=-1)\n            target_matrix_flatten = torch.flatten(target_matrix, start_dim=1,end_dim=-1)\n            output_2d_loss = criterion(output_2d_flatten, target_matrix_flatten)\n            loss_components.append(output_2d_loss)\n        else:\n            # Use a small multiplier on the output to ensure gradients flow\n            output_2d_loss = 0.0 * output_2d.mean()\n            loss_components.append(output_2d_loss)\n            \n        if target_vector is not None:\n            output_1d_loss = criterion(output_1d, target_vector)\n            loss_components.append(output_1d_loss)\n        else:\n            # Use a small multiplier on the output to ensure gradients flow\n            output_1d_loss = 0.0 * output_1d.mean()\n            loss_components.append(output_1d_loss)\n        \n        # Sum all loss components\n        loss = sum(loss_components)\n        \n        # Update metrics\n        metric_logger.update(loss=to_value(loss))\n        metric_logger.update(embedding_loss=to_value(embedding_loss))\n        metric_logger.update(output_2d_loss=to_value(output_2d_loss))\n        metric_logger.update(output_1d_loss=to_value(output_1d_loss))\n        \n        if not math.isfinite(to_value(loss)):\n            print(\"Loss is {}, stopping training\".format(to_value(loss)))\n            #sys.exit(1)\n            optimizer.zero_grad()\n            continue\n            \n        loss = loss / accum_iter\n        loss_scaler(loss, optimizer, parameters=model.parameters(),\n                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n\n        if (data_iter_step + 1) % accum_iter == 0:\n            optimizer.zero_grad()\n\n        torch.cuda.synchronize() # Make sure all gradients are finished computing before moving on\n        lr = optimizer.param_groups[0][\"lr\"]\n        metric_logger.update(lr=lr)\n        \n\n        if log_writer is not None and ((data_iter_step + 1) % accum_iter == 0 or data_iter_step==0):\n            \"\"\" \n            We use epoch_1000x as the x-axis in tensorboard.\n            This calibrates different curves when batch size changes.\n            \"\"\"\n            epoch_1000x = int((data_iter_step / len(data_loader_train) + epoch) * 1000)\n            log_writer.add_scalars('Loss/loss', {'train_loss': to_value(loss)}, epoch_1000x)\n            log_writer.add_scalars('Loss/embedding_loss', {'train_loss': to_value(embedding_loss)}, epoch_1000x)\n            log_writer.add_scalars('Loss/output_2d_loss', {'train_loss': to_value(output_2d_loss)}, epoch_1000x)\n            log_writer.add_scalars('Loss/output_1d_loss', {'train_loss': to_value(output_1d_loss)}, epoch_1000x)\n            log_writer.add_scalars('LR/lr', {'lr': lr}, epoch_1000x)\n            if ((data_iter_step+1)//accum_iter)%50==0 or data_iter_step==0:\n                #add visualization for your output and input\n                new_samples = create_image(input_matrix)\n                select_num = min(8,len(new_samples))\n                sample_image = torch_to_nparray(new_samples.clone().detach()[:select_num])\n                log_writer.add_images('Input_%s'%\"train\", sample_image, epoch_1000x)\n                output_2d_image = convert_gray_rgbimage(output_2d.clone().detach()[:select_num])\n                output_2d_image = torch_to_nparray(output_2d_image)\n                log_writer.add_images('Output_2d_%s'%\"train\", output_2d_image, epoch_1000x)\n                # for name, param in model.named_parameters():\n                #     log_writer.add_histogram(name, param, epoch_1000x)\n                #raise errors, see https://github.com/pytorch/pytorch/issues/91516\n                #If you want to use this, install tensorboardX \n                #then change the code in main_worker.py to \"from tensorboardX import SummaryWriter\"\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9b66f075-0380-4f66-961b-c5e759d5273b",
      "cell_type": "markdown",
      "source": "Run Fine-tuning",
      "metadata": {}
    },
    {
      "id": "148fb5da-a685-4f14-84ab-e6d782e057f8",
      "cell_type": "code",
      "source": "!python3 finetune.py --batch_size 1 --accum_iter 4 \\\n    --epochs 1 --warmup_epochs 0 --pin_mem \\\n    --blr 1e-3 --min_lr 1e-7 --weight_decay 0.05 \\\n    --layer_decay 0.75 --model vit_large_patch16 \\\n    --pretrain hicfoundation_pretrain/model/model_best.pth.tar \\\n    --finetune 1 --seed 888 \\\n    --loss_type 1 --data_path \"ft-inputs\" \\\n    --train_config \"train_config.txt\" \\\n    --valid_config \"val_config.txt\" \\\n    --output \"hicfoundation_finetune\" --tensorboard 1 \\\n    --world_size 1 --dist_url \"tcp://localhost:10001\" --rank 0 \\\n    --input_row_size 448 --input_col_size 448 --patch_size 16 \\\n    --print_freq 1 --save_freq 1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "084d606c-0c13-4de9-b4ff-a8dbb05f3207",
      "cell_type": "markdown",
      "source": "8. Inference\nRun Inference",
      "metadata": {}
    },
    {
      "id": "dd935ce0-bd40-46ae-9ecc-0fd916aff767",
      "cell_type": "code",
      "source": "# Update the filename below to match your uploaded test file\n!python inference.py --batch_size 1 \\\n    --input hic-raw/B1-GSM4705442_cmt2cmt3.hic \\\n    --resolution 10000 \\\n    --task 3 \\\n    --input_row_size 224 --input_col_size 224 \\\n    --stride 32 --bound 0 \\\n    --num_workers 1 \\\n    --model hicfoundation_finetune/model/model_best.pth.tar \\\n    --model_path hicfoundation_finetune/model/model_best.pth.tar \\\n    --output outputs/B1_enhanced",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "36c8a9f4-b428-47b1-bc95-4a4c12ae3139",
      "cell_type": "markdown",
      "source": "9. Visualization and Analysis\nVisualization Functions",
      "metadata": {}
    },
    {
      "id": "fb15cd82-4af5-40ba-a4b1-e7ab52f61c09",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr, spearmanr\n\ndef visualize_hic_matrix(matrix, title=\"Hi-C Contact Matrix\", cmap='Reds', vmax=None):\n    \"\"\"\n    Visualize a Hi-C contact matrix\n    \"\"\"\n    plt.figure(figsize=(10, 10))\n    if vmax is None:\n        vmax = np.percentile(matrix[matrix > 0], 95)\n    plt.imshow(matrix, cmap=cmap, vmin=0, vmax=vmax, interpolation='none')\n    plt.colorbar(label='Contact Frequency')\n    plt.title(title)\n    plt.xlabel('Genomic Position')\n    plt.ylabel('Genomic Position')\n    plt.show()\n\ndef compare_matrices(low_res, high_res, enhanced, region_name=\"\"):\n    \"\"\"\n    Compare low resolution, high resolution, and enhanced matrices\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    \n    # Calculate common color scale\n    vmax = np.percentile(np.concatenate([\n        low_res[low_res > 0],\n        high_res[high_res > 0],\n        enhanced[enhanced > 0]\n    ]), 95)\n    \n    # Plot low resolution\n    im1 = axes[0].imshow(low_res, cmap='Reds', vmin=0, vmax=vmax, interpolation='none')\n    axes[0].set_title(f'Low Resolution\\n{region_name}')\n    axes[0].set_xlabel('Genomic Position')\n    axes[0].set_ylabel('Genomic Position')\n    \n    # Plot high resolution (ground truth)\n    im2 = axes[1].imshow(high_res, cmap='Reds', vmin=0, vmax=vmax, interpolation='none')\n    axes[1].set_title(f'High Resolution (Ground Truth)\\n{region_name}')\n    axes[1].set_xlabel('Genomic Position')\n    \n    # Plot enhanced\n    im3 = axes[2].imshow(enhanced, cmap='Reds', vmin=0, vmax=vmax, interpolation='none')\n    axes[2].set_title(f'Enhanced Resolution\\n{region_name}')\n    axes[2].set_xlabel('Genomic Position')\n    \n    # Add colorbar\n    fig.colorbar(im3, ax=axes, label='Contact Frequency', fraction=0.046, pad=0.04)\n    \n    plt.tight_layout()\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "04e8daa7-9111-49f0-9e6d-c3e90626ee26",
      "cell_type": "markdown",
      "source": "Load and Visualize Results",
      "metadata": {}
    },
    {
      "id": "e088a33c-a447-45c8-80aa-281be8917c78",
      "cell_type": "code",
      "source": "# Load enhanced results\noutput_pkl = 'outputs/B1_enhanced/HiCFoundation_enhanced.pkl'\n\nif os.path.exists(output_pkl):\n    with open(output_pkl, 'rb') as f:\n        enhanced_data = pickle.load(f)\n    \n    print(\"Enhanced Hi-C data loaded successfully!\")\n    print(f\"Chromosomes available: {list(enhanced_data.keys())}\")\n    \n    # Visualize a sample chromosome\n    sample_chrom = list(enhanced_data.keys())[0]\n    sample_matrix = enhanced_data[sample_chrom]\n    \n    if hasattr(sample_matrix, 'toarray'):\n        sample_matrix = sample_matrix.toarray()\n    \n    visualize_hic_matrix(sample_matrix[:500, :500], \n                        title=f\"Enhanced Hi-C - {sample_chrom} (500x500 region)\")\nelse:\n    print(f\"Output file not found: {output_pkl}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1241795f-6900-4708-8ba2-8c6fa302b363",
      "cell_type": "markdown",
      "source": "Download Results",
      "metadata": {}
    },
    {
      "id": "6712b1a8-d1d3-40e1-a4fe-e9830fb4cd2e",
      "cell_type": "code",
      "source": "# Zip and download results\nimport zipfile\n\nzip_filename = 'hicfoundation_results.zip'\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    for root, dirs, files in os.walk('outputs'):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zipf.write(file_path, os.path.relpath(file_path, '.'))\n\n# Download the zip file\nfiles.download(zip_filename)\nprint(f\"Results downloaded as {zip_filename}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a9503283-c4dc-4049-813e-1f45ee3912cb",
      "cell_type": "markdown",
      "source": "Done! I apologize if there is any errors. This is my first time using Jupyter Notebook. ",
      "metadata": {}
    }
  ]
}